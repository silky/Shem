[const fst map drop length at drop either id concat] (require Tea.Prelude [const fst map drop length at drop either id concat])
[match-regex join-regex] (require Tea.Regex [match-regex join-regex])

Token (type {TokenType TokenValue TokenPos})

TokenType (data
  ;open-paren
  ;closed-paren
  ;open-bracket
  ;closed-bracket
  ;symbol
  ;whitespace
  ;regex
  ;string
  ;char)

TokenValue (type String)

TokenPos (type Int)

TokenRule (type (Tuple Regex Token))

tokenization-rules [
  {/\s+/ ;whitespace}
  {/\(/ ;open-paren}
  {/\)/ ;closed-paren}
  {/\[/ ;open-bracket}
  {/\]/ ;closed-bracket}
  {/"[^"]*?"/ ;string}
  {/'\\?[^']'/ ;char}
  {/[^\(\)\[\]"'\s]+/ ;symbol}]

tokenize (fn [source]
  (tokenize-at 0 tokenization-rules source))

tokenize-at (fn [pos rules source]
  (:: String (List TokenRule) (Either (List Token) TokenError))
  (match possible-token
    {:just {kind value}} (match (tokenize-at (+ (length value) pos) rules rest)
      {:good tokens} {:good (& {kind value pos} tokens)}
      error error)
    ;none (if (= "" rest)
      {:good (list)}
      {:bad {:error rest}}))
  {possible-token rest} (match-token rules-to-apply source)
  rules-to-apply (map rule-application rules))

rule-application (fn [rule string]
  (:: TokenRule TokenRuleApplication)
  (match (match-regex regex-at-start string)
    {:just token} {:just {kind token (drop (length token) string)}}
    none none)
  {regex kind} rule
  regex-at-start (join-regex /^/ regex))

TokenRuleApplication (type (String (Maybe {TokenType TokenValue String})))

match-token (fn [rules source]
  (:: (List TokenRuleApplication) String {(Maybe { TokenType TokenValue}) String})
  (match (find-first rules source)
    ;none {;none source}
    {:just {kind value rest}} {{:just {kind value}} rest}))

find-first (fn [funs input]
  (:: (List TokenRuleApplication) String (Maybe {TokenType TokenValue String}))
  (match funs
    [] ;none
    [f ..rest] (match (f input)
      ;none (find-first rest input)
      found found)))

print-tokens (fn [tokens]
  (either show-list id tokens))

de-tokenize (fn [tokens]
  (concat (either (map token-value) id tokens)))

token-value (fn [token]
  value
  {_ value _} token)
